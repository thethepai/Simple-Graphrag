19:45:37,103 graphrag.index.cli INFO Logging enabled at graphrag-files\output\20240907-194537\reports\indexing-engine.log
19:45:37,105 graphrag.index.cli INFO Starting pipeline run for: 20240907-194537, dryrun=False
19:45:37,106 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "graphrag-files",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
19:45:37,109 graphrag.index.create_pipeline_config INFO skipping workflows 
19:45:37,109 graphrag.index.run INFO Running pipeline
19:45:37,109 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at graphrag-files\output\20240907-194537\artifacts
19:45:37,111 graphrag.index.input.load_input INFO loading input from root_dir=input
19:45:37,111 graphrag.index.input.load_input INFO using file storage for input
19:45:37,112 graphrag.index.storage.file_pipeline_storage INFO search graphrag-files\input for files matching .*\.txt$
19:45:37,113 graphrag.index.input.text INFO found text files from input, found [('combined.txt', {})]
19:45:37,115 graphrag.index.input.text INFO Found 1 files, loading 1
19:45:37,117 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
19:45:37,118 graphrag.index.run INFO Final # of rows loaded: 1
19:45:37,244 graphrag.index.run INFO Running workflow: create_base_text_units...
19:45:37,245 graphrag.index.run INFO dependencies for create_base_text_units: []
19:45:37,249 datashaper.workflow.workflow INFO executing verb orderby
19:45:37,252 datashaper.workflow.workflow INFO executing verb zip
19:45:37,256 datashaper.workflow.workflow INFO executing verb aggregate_override
19:45:37,266 datashaper.workflow.workflow INFO executing verb chunk
19:45:37,427 datashaper.workflow.workflow INFO executing verb select
19:45:37,433 datashaper.workflow.workflow INFO executing verb unroll
19:45:37,443 datashaper.workflow.workflow INFO executing verb rename
19:45:37,446 datashaper.workflow.workflow INFO executing verb genid
19:45:37,451 datashaper.workflow.workflow INFO executing verb unzip
19:45:37,455 datashaper.workflow.workflow INFO executing verb copy
19:45:37,459 datashaper.workflow.workflow INFO executing verb filter
19:45:37,471 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:45:37,660 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
19:45:37,661 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
19:45:37,661 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:45:37,705 datashaper.workflow.workflow INFO executing verb entity_extract
19:45:37,715 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
19:45:38,402 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
19:45:38,402 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
19:46:08,246 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:08,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.81200000003446. input_tokens=3283, output_tokens=888
19:46:23,815 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:23,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.35999999998603. input_tokens=3284, output_tokens=1432
19:46:29,490 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:29,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.01500000001397. input_tokens=2536, output_tokens=1612
19:46:34,899 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:34,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.079000000027008. input_tokens=34, output_tokens=205
19:46:36,516 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:36,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.078000000095926. input_tokens=3283, output_tokens=1743
19:46:36,849 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:36,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.39099999994505. input_tokens=3284, output_tokens=1790
19:46:42,186 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:42,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.93799999996554. input_tokens=34, output_tokens=978
19:46:42,690 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:42,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.21899999992456. input_tokens=3284, output_tokens=1972
19:46:44,444 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:44,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.06299999996554. input_tokens=3286, output_tokens=2119
19:46:44,917 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:44,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.51500000001397. input_tokens=3283, output_tokens=2111
19:46:45,212 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:45,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.76600000006147. input_tokens=3283, output_tokens=2181
19:46:47,597 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:47,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.20300000009593. input_tokens=3284, output_tokens=2186
19:46:50,781 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:50,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.34399999992456. input_tokens=3284, output_tokens=2362
19:46:51,1 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:51,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.625. input_tokens=3284, output_tokens=2318
19:46:53,639 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:53,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.18800000008196. input_tokens=3283, output_tokens=2571
19:46:54,365 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:54,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.875. input_tokens=34, output_tokens=544
19:46:54,483 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:46:54,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.06300000008196. input_tokens=3283, output_tokens=2438
19:47:00,327 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:00,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.89099999994505. input_tokens=3284, output_tokens=2519
19:47:01,224 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:01,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.76599999994505. input_tokens=3284, output_tokens=2609
19:47:04,179 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:04,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 85.75. input_tokens=3285, output_tokens=2824
19:47:14,562 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:14,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 96.17200000002049. input_tokens=3285, output_tokens=3075
19:47:17,330 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:17,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.375. input_tokens=34, output_tokens=460
19:47:19,949 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:19,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.14000000001397. input_tokens=34, output_tokens=540
19:47:20,550 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:20,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 102.10900000005495. input_tokens=3284, output_tokens=3529
19:47:22,907 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:22,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.54700000002049. input_tokens=34, output_tokens=365
19:47:24,950 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:24,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.46799999999348. input_tokens=34, output_tokens=838
19:47:27,310 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:27,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.06299999996554. input_tokens=34, output_tokens=475
19:47:31,521 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:31,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.01599999994505. input_tokens=34, output_tokens=894
19:47:38,235 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:38,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.01599999994505. input_tokens=34, output_tokens=1050
19:47:43,585 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:43,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.73400000005495. input_tokens=34, output_tokens=1440
19:47:44,185 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:44,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.14099999994505. input_tokens=34, output_tokens=1004
19:47:59,163 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:47:59,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.53099999995902. input_tokens=34, output_tokens=1678
19:48:15,232 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:15,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.01599999994505. input_tokens=34, output_tokens=1692
19:48:20,153 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:20,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 97.45400000002701. input_tokens=34, output_tokens=1959
19:48:21,130 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:21,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 87.53099999995902. input_tokens=34, output_tokens=1822
19:48:32,925 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:32,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 98.42199999990407. input_tokens=34, output_tokens=2048
19:48:35,86 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:35,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.5. input_tokens=34, output_tokens=1694
19:48:53,7 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:48:53,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 92.45300000009593. input_tokens=34, output_tokens=2124
19:48:53,22 datashaper.workflow.workflow INFO executing verb merge_graphs
19:48:53,42 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
19:48:53,185 graphrag.index.run INFO Running workflow: create_final_covariates...
19:48:53,186 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
19:48:53,186 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:48:53,199 datashaper.workflow.workflow INFO executing verb extract_covariates
19:49:05,869 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:05,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.625. input_tokens=2314, output_tokens=331
19:49:06,726 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:06,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.468999999924563. input_tokens=2314, output_tokens=330
19:49:10,52 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:10,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.81299999996554. input_tokens=2314, output_tokens=470
19:49:11,76 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:11,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.79599999997299. input_tokens=1567, output_tokens=549
19:49:13,545 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:13,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.26500000001397. input_tokens=2315, output_tokens=546
19:49:19,968 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:19,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.70299999997951. input_tokens=2314, output_tokens=970
19:49:20,298 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:20,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.030999999959022. input_tokens=2314, output_tokens=842
19:49:20,699 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:20,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.46799999999348. input_tokens=2315, output_tokens=903
19:49:22,247 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:22,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.0. input_tokens=2313, output_tokens=883
19:49:24,445 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:24,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.21799999999348. input_tokens=2314, output_tokens=999
19:49:24,516 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:24,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.25. input_tokens=2313, output_tokens=921
19:49:24,903 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:24,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.65700000000652. input_tokens=2314, output_tokens=889
19:49:25,353 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:25,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.125. input_tokens=2316, output_tokens=947
19:49:26,634 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:26,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.42200000002049. input_tokens=2315, output_tokens=1059
19:49:27,571 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:27,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.34299999999348. input_tokens=2314, output_tokens=1041
19:49:28,928 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:28,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.65599999995902. input_tokens=2315, output_tokens=1006
19:49:35,177 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:35,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.95299999997951. input_tokens=2315, output_tokens=1144
19:49:35,420 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:35,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.15599999995902. input_tokens=2315, output_tokens=1357
19:49:43,34 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:43,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.15700000000652. input_tokens=19, output_tokens=1023
19:49:45,137 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:45,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.594000000040978. input_tokens=19, output_tokens=955
19:49:49,759 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:49,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.703000000095926. input_tokens=19, output_tokens=1205
19:49:52,726 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:52,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.82799999997951. input_tokens=19, output_tokens=669
19:49:53,899 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:53,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.59400000004098. input_tokens=19, output_tokens=1053
19:49:54,75 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:54,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.82799999997951. input_tokens=19, output_tokens=973
19:49:56,842 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:56,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.906000000075437. input_tokens=19, output_tokens=715
19:49:57,71 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:49:57,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.0. input_tokens=19, output_tokens=1483
19:50:03,179 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:03,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.546999999904074. input_tokens=19, output_tokens=1030
19:50:06,318 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:06,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.75. input_tokens=19, output_tokens=1175
19:50:08,778 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:08,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.59399999992456. input_tokens=2314, output_tokens=2457
19:50:09,614 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:09,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.89000000001397. input_tokens=19, output_tokens=1702
19:50:12,792 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:12,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.28099999995902. input_tokens=19, output_tokens=1382
19:50:12,802 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:12,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.10999999998603. input_tokens=19, output_tokens=1556
19:50:13,556 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:13,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.10999999998603. input_tokens=19, output_tokens=1481
19:50:15,115 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:15,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.76500000001397. input_tokens=19, output_tokens=1252
19:50:18,175 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:18,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.375. input_tokens=19, output_tokens=206
19:50:23,124 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:23,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.93700000003446. input_tokens=19, output_tokens=1395
19:50:27,686 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:27,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.26599999994505. input_tokens=19, output_tokens=1608
19:50:38,750 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:38,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.78099999995902. input_tokens=19, output_tokens=2857
19:50:38,766 datashaper.workflow.workflow INFO executing verb window
19:50:38,771 datashaper.workflow.workflow INFO executing verb genid
19:50:38,776 datashaper.workflow.workflow INFO executing verb convert
19:50:38,790 datashaper.workflow.workflow INFO executing verb rename
19:50:38,796 datashaper.workflow.workflow INFO executing verb select
19:50:38,798 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
19:50:38,961 graphrag.index.run INFO Running workflow: create_summarized_entities...
19:50:38,962 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
19:50:38,962 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
19:50:38,978 datashaper.workflow.workflow INFO executing verb summarize_descriptions
19:50:41,150 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.062999999965541. input_tokens=292, output_tokens=49
19:50:41,243 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.202999999979511. input_tokens=307, output_tokens=50
19:50:41,268 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.172000000020489. input_tokens=290, output_tokens=45
19:50:41,464 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3910000000614673. input_tokens=291, output_tokens=52
19:50:41,660 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.577999999979511. input_tokens=291, output_tokens=55
19:50:41,747 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.655999999959022. input_tokens=293, output_tokens=62
19:50:41,809 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.718999999924563. input_tokens=278, output_tokens=149
19:50:41,852 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:41,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.765999999945052. input_tokens=295, output_tokens=69
19:50:42,219 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1410000000614673. input_tokens=351, output_tokens=70
19:50:42,261 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.172000000020489. input_tokens=299, output_tokens=78
19:50:42,375 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.297000000020489. input_tokens=396, output_tokens=82
19:50:42,468 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3910000000614673. input_tokens=328, output_tokens=77
19:50:42,523 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.48499999998603. input_tokens=333, output_tokens=170
19:50:42,592 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5160000000614673. input_tokens=419, output_tokens=76
19:50:42,847 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:42,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.797000000020489. input_tokens=587, output_tokens=85
19:50:43,20 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.969000000040978. input_tokens=572, output_tokens=91
19:50:43,266 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.203000000095926. input_tokens=569, output_tokens=101
19:50:43,613 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=303, output_tokens=57
19:50:43,638 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.563000000081956. input_tokens=383, output_tokens=100
19:50:43,686 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.640999999945052. input_tokens=338, output_tokens=129
19:50:43,767 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.688000000081956. input_tokens=449, output_tokens=112
19:50:43,787 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.75. input_tokens=513, output_tokens=104
19:50:43,907 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.844000000040978. input_tokens=512, output_tokens=131
19:50:43,957 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:43,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.812000000034459. input_tokens=288, output_tokens=70
19:50:44,82 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:44,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.031000000075437. input_tokens=621, output_tokens=123
19:50:44,896 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:44,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.312999999965541. input_tokens=316, output_tokens=53
19:50:44,909 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:44,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.844000000040978. input_tokens=598, output_tokens=140
19:50:45,2 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7339999999385327. input_tokens=309, output_tokens=98
19:50:45,6 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.75. input_tokens=350, output_tokens=70
19:50:45,62 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.530999999959022. input_tokens=284, output_tokens=56
19:50:45,219 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.406000000075437. input_tokens=309, output_tokens=90
19:50:45,253 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.030999999959022. input_tokens=296, output_tokens=83
19:50:45,267 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.234000000054948. input_tokens=347, output_tokens=53
19:50:45,694 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8429999999934807. input_tokens=315, output_tokens=94
19:50:45,811 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.062999999965541. input_tokens=306, output_tokens=98
19:50:45,863 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.390999999945052. input_tokens=308, output_tokens=90
19:50:45,925 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.468999999924563. input_tokens=308, output_tokens=115
19:50:45,967 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.109000000054948. input_tokens=331, output_tokens=85
19:50:45,984 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:45,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.187999999965541. input_tokens=277, output_tokens=58
19:50:46,26 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.344000000040978. input_tokens=347, output_tokens=60
19:50:46,126 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999959022. input_tokens=317, output_tokens=39
19:50:46,234 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.968999999924563. input_tokens=345, output_tokens=80
19:50:46,249 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4839999999385327. input_tokens=297, output_tokens=157
19:50:46,357 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.312999999965541. input_tokens=578, output_tokens=144
19:50:46,387 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.484000000054948. input_tokens=303, output_tokens=60
19:50:46,480 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.422000000020489. input_tokens=310, output_tokens=29
19:50:46,591 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.969000000040978. input_tokens=345, output_tokens=64
19:50:46,909 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:46,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9070000000065193. input_tokens=350, output_tokens=75
19:50:47,63 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.390999999945052. input_tokens=407, output_tokens=147
19:50:47,247 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.875. input_tokens=303, output_tokens=82
19:50:47,267 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.625. input_tokens=371, output_tokens=103
19:50:47,420 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.327999999979511. input_tokens=322, output_tokens=92
19:50:47,480 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.515999999945052. input_tokens=319, output_tokens=38
19:50:47,666 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.76500000001397. input_tokens=383, output_tokens=62
19:50:47,840 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:47,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1410000000614673. input_tokens=338, output_tokens=53
19:50:48,192 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9369999999180436. input_tokens=311, output_tokens=80
19:50:48,652 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.437999999965541. input_tokens=322, output_tokens=92
19:50:48,718 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.702999999979511. input_tokens=349, output_tokens=99
19:50:48,799 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.734000000054948. input_tokens=349, output_tokens=40
19:50:48,866 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=343, output_tokens=41
19:50:48,925 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:48,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5469999999040738. input_tokens=336, output_tokens=70
19:50:49,24 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.437999999965541. input_tokens=368, output_tokens=137
19:50:49,150 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0320000000065193. input_tokens=350, output_tokens=78
19:50:49,475 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.577999999979511. input_tokens=384, output_tokens=67
19:50:49,631 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.609000000054948. input_tokens=388, output_tokens=90
19:50:49,633 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.391000000061467. input_tokens=315, output_tokens=112
19:50:49,751 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.842999999993481. input_tokens=378, output_tokens=136
19:50:49,842 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.906000000075437. input_tokens=315, output_tokens=103
19:50:49,911 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.655999999959022. input_tokens=354, output_tokens=150
19:50:49,920 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:49,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=364, output_tokens=102
19:50:50,37 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:50,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.687000000034459. input_tokens=344, output_tokens=92
19:50:50,121 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:50,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.14000000001397. input_tokens=395, output_tokens=109
19:50:50,178 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:50,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.952999999979511. input_tokens=311, output_tokens=101
19:50:50,260 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:50,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.016000000061467. input_tokens=376, output_tokens=85
19:50:50,345 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:50,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.531000000075437. input_tokens=355, output_tokens=254
19:50:51,12 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.141000000061467. input_tokens=331, output_tokens=140
19:50:51,292 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.64000000001397. input_tokens=340, output_tokens=156
19:50:51,419 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.702999999979511. input_tokens=324, output_tokens=80
19:50:51,421 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.577999999979511. input_tokens=415, output_tokens=104
19:50:51,666 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.187000000034459. input_tokens=319, output_tokens=137
19:50:51,734 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.937999999965541. input_tokens=383, output_tokens=63
19:50:51,912 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:51,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5. input_tokens=383, output_tokens=120
19:50:52,194 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:52,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1709999999729916. input_tokens=334, output_tokens=147
19:50:52,257 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:52,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.063000000081956. input_tokens=428, output_tokens=100
19:50:52,309 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:52,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.437999999965541. input_tokens=350, output_tokens=47
19:50:52,319 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:52,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.842999999993481. input_tokens=390, output_tokens=131
19:50:52,611 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:52,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.672000000020489. input_tokens=354, output_tokens=107
19:50:53,777 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:50:53,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.625. input_tokens=359, output_tokens=117
19:50:53,793 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
19:50:53,941 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
19:50:53,942 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
19:50:53,942 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
19:50:53,961 datashaper.workflow.workflow INFO executing verb select
19:50:53,969 datashaper.workflow.workflow INFO executing verb aggregate_override
19:50:53,976 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
19:50:54,129 graphrag.index.run INFO Running workflow: create_base_entity_graph...
19:50:54,130 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
19:50:54,130 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
19:50:54,148 datashaper.workflow.workflow INFO executing verb cluster_graph
19:50:54,202 datashaper.workflow.workflow INFO executing verb select
19:50:54,204 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:50:54,355 graphrag.index.run INFO Running workflow: create_final_entities...
19:50:54,356 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:50:54,356 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
19:50:54,375 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:54,395 datashaper.workflow.workflow INFO executing verb rename
19:50:54,404 datashaper.workflow.workflow INFO executing verb select
19:50:54,412 datashaper.workflow.workflow INFO executing verb dedupe
19:50:54,423 datashaper.workflow.workflow INFO executing verb rename
19:50:54,432 datashaper.workflow.workflow INFO executing verb filter
19:50:54,453 datashaper.workflow.workflow INFO executing verb text_split
19:50:54,464 datashaper.workflow.workflow INFO executing verb drop
19:50:54,474 datashaper.workflow.workflow INFO executing verb merge
19:50:54,500 datashaper.workflow.workflow INFO executing verb text_embed
19:50:54,504 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
19:50:55,202 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
19:50:55,202 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
19:50:55,210 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 112 inputs via 112 snippets using 7 batches. max_batch_size=16, max_tokens=8191
19:50:55,948 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:55,978 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:56,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0939999999245629. input_tokens=557, output_tokens=0
19:50:56,328 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:56,328 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:56,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2339999999385327. input_tokens=406, output_tokens=0
19:50:56,461 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:56,463 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:56,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.437999999965541. input_tokens=307, output_tokens=0
19:50:56,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.577999999979511. input_tokens=1000, output_tokens=0
19:50:56,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.702999999979511. input_tokens=1227, output_tokens=0
19:50:56,932 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
19:50:57,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8439999999245629. input_tokens=907, output_tokens=0
19:50:57,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9839999999385327. input_tokens=1711, output_tokens=0
19:50:57,242 datashaper.workflow.workflow INFO executing verb drop
19:50:57,252 datashaper.workflow.workflow INFO executing verb filter
19:50:57,268 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:50:57,455 graphrag.index.run INFO Running workflow: create_final_nodes...
19:50:57,456 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:50:57,457 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
19:50:57,481 datashaper.workflow.workflow INFO executing verb layout_graph
19:50:57,537 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:57,560 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:57,585 datashaper.workflow.workflow INFO executing verb filter
19:50:57,622 datashaper.workflow.workflow INFO executing verb drop
19:50:57,635 datashaper.workflow.workflow INFO executing verb select
19:50:57,648 datashaper.workflow.workflow INFO executing verb rename
19:50:57,661 datashaper.workflow.workflow INFO executing verb join
19:50:57,683 datashaper.workflow.workflow INFO executing verb convert
19:50:57,725 datashaper.workflow.workflow INFO executing verb rename
19:50:57,728 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:50:57,899 graphrag.index.run INFO Running workflow: create_final_communities...
19:50:57,900 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:50:57,900 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
19:50:57,929 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:57,952 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:57,977 datashaper.workflow.workflow INFO executing verb aggregate_override
19:50:57,994 datashaper.workflow.workflow INFO executing verb join
19:50:58,13 datashaper.workflow.workflow INFO executing verb join
19:50:58,32 datashaper.workflow.workflow INFO executing verb concat
19:50:58,46 datashaper.workflow.workflow INFO executing verb filter
19:50:58,89 datashaper.workflow.workflow INFO executing verb aggregate_override
19:50:58,107 datashaper.workflow.workflow INFO executing verb join
19:50:58,125 datashaper.workflow.workflow INFO executing verb filter
19:50:58,158 datashaper.workflow.workflow INFO executing verb fill
19:50:58,173 datashaper.workflow.workflow INFO executing verb merge
19:50:58,193 datashaper.workflow.workflow INFO executing verb copy
19:50:58,208 datashaper.workflow.workflow INFO executing verb select
19:50:58,210 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:50:58,389 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
19:50:58,390 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
19:50:58,391 graphrag.index.run INFO read table from storage: create_final_entities.parquet
19:50:58,429 datashaper.workflow.workflow INFO executing verb select
19:50:58,445 datashaper.workflow.workflow INFO executing verb unroll
19:50:58,463 datashaper.workflow.workflow INFO executing verb aggregate_override
19:50:58,467 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
19:50:58,653 graphrag.index.run INFO Running workflow: create_final_relationships...
19:50:58,657 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
19:50:58,658 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
19:50:58,663 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
19:50:58,698 datashaper.workflow.workflow INFO executing verb unpack_graph
19:50:58,727 datashaper.workflow.workflow INFO executing verb filter
19:50:58,765 datashaper.workflow.workflow INFO executing verb rename
19:50:58,782 datashaper.workflow.workflow INFO executing verb filter
19:50:58,822 datashaper.workflow.workflow INFO executing verb drop
19:50:58,839 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
19:50:58,860 datashaper.workflow.workflow INFO executing verb convert
19:50:58,897 datashaper.workflow.workflow INFO executing verb convert
19:50:58,899 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:50:59,80 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
19:50:59,80 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
19:50:59,81 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
19:50:59,120 datashaper.workflow.workflow INFO executing verb select
19:50:59,139 datashaper.workflow.workflow INFO executing verb unroll
19:50:59,159 datashaper.workflow.workflow INFO executing verb aggregate_override
19:50:59,180 datashaper.workflow.workflow INFO executing verb select
19:50:59,182 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
19:50:59,360 graphrag.index.run INFO Running workflow: create_final_community_reports...
19:50:59,361 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes', 'create_final_covariates']
19:50:59,361 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
19:50:59,366 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
19:50:59,370 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
19:50:59,411 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
19:50:59,433 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
19:50:59,455 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
19:50:59,476 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
19:50:59,500 datashaper.workflow.workflow INFO executing verb prepare_community_reports
19:50:59,501 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 112
19:50:59,543 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 112
19:50:59,626 datashaper.workflow.workflow INFO executing verb create_community_reports
19:51:18,595 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:18,596 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:18,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.891000000061467. input_tokens=1902, output_tokens=532
19:51:19,139 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:19,139 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:19,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.484000000054948. input_tokens=2210, output_tokens=549
19:51:20,234 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:20,235 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:20,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.57799999997951. input_tokens=2276, output_tokens=583
19:51:21,461 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:21,461 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:21,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.766000000061467. input_tokens=2689, output_tokens=573
19:51:21,590 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:21,591 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:21,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.95299999997951. input_tokens=2241, output_tokens=617
19:51:21,829 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:21,830 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:21,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.125. input_tokens=3003, output_tokens=588
19:51:23,121 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:23,122 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:23,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.43700000003446. input_tokens=2941, output_tokens=593
19:51:24,23 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:24,24 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:24,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.375. input_tokens=3053, output_tokens=604
19:51:26,659 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:26,660 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:26,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.969000000040978. input_tokens=2214, output_tokens=657
19:51:27,231 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:27,232 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:27,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.54700000002049. input_tokens=3875, output_tokens=718
19:51:48,340 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:48,341 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:48,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.04700000002049. input_tokens=2711, output_tokens=584
19:51:49,304 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:49,305 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:49,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.046999999904074. input_tokens=3586, output_tokens=602
19:51:50,626 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:50,627 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:50,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.32799999997951. input_tokens=2213, output_tokens=644
19:51:52,563 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:52,564 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:52,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.280999999959022. input_tokens=2931, output_tokens=611
19:51:53,839 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:53,840 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:53,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.56200000003446. input_tokens=5070, output_tokens=464
19:51:54,116 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:54,117 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:54,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.82799999997951. input_tokens=2437, output_tokens=734
19:51:56,852 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:56,852 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:56,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.57799999997951. input_tokens=3959, output_tokens=547
19:51:58,692 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:51:58,693 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:51:58,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.42099999997299. input_tokens=3804, output_tokens=579
19:52:01,402 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
19:52:01,402 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:52:01,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.14099999994505. input_tokens=3581, output_tokens=523
19:52:01,453 datashaper.workflow.workflow INFO executing verb window
19:52:01,457 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:52:01,678 graphrag.index.run INFO Running workflow: create_final_text_units...
19:52:01,679 graphrag.index.run INFO dependencies for create_final_text_units: ['create_base_text_units', 'join_text_units_to_covariate_ids', 'join_text_units_to_relationship_ids', 'join_text_units_to_entity_ids']
19:52:01,679 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:52:01,683 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
19:52:01,686 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
19:52:01,688 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
19:52:01,732 datashaper.workflow.workflow INFO executing verb select
19:52:01,753 datashaper.workflow.workflow INFO executing verb rename
19:52:01,775 datashaper.workflow.workflow INFO executing verb join
19:52:01,801 datashaper.workflow.workflow INFO executing verb join
19:52:01,827 datashaper.workflow.workflow INFO executing verb join
19:52:01,852 datashaper.workflow.workflow INFO executing verb aggregate_override
19:52:01,876 datashaper.workflow.workflow INFO executing verb select
19:52:01,879 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:52:02,71 graphrag.index.run INFO Running workflow: create_base_documents...
19:52:02,72 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
19:52:02,72 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
19:52:02,120 datashaper.workflow.workflow INFO executing verb unroll
19:52:02,143 datashaper.workflow.workflow INFO executing verb select
19:52:02,166 datashaper.workflow.workflow INFO executing verb rename
19:52:02,190 datashaper.workflow.workflow INFO executing verb join
19:52:02,216 datashaper.workflow.workflow INFO executing verb aggregate_override
19:52:02,241 datashaper.workflow.workflow INFO executing verb join
19:52:02,269 datashaper.workflow.workflow INFO executing verb rename
19:52:02,292 datashaper.workflow.workflow INFO executing verb convert
19:52:02,320 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
19:52:02,503 graphrag.index.run INFO Running workflow: create_final_documents...
19:52:02,527 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
19:52:02,527 graphrag.index.run INFO read table from storage: create_base_documents.parquet
19:52:02,579 datashaper.workflow.workflow INFO executing verb rename
19:52:02,582 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:52:02,667 graphrag.index.cli INFO All workflows completed successfully.
